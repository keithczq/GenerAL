
<center>
  <p>
    <a target="_blank" href="http://crlab.cs.columbia.edu/GenerAL/">Bohan Wu<span class="glyphicon glyphicon-new-window" aria-hidden="true"></span></a>, 
    <a target="_blank" href="http://www.cs.columbia.edu/~iakinola/">Iretiayo Akinola<span class="glyphicon glyphicon-new-window" aria-hidden="true"></span></a>, 
    <a target="_blank" href="http://crlab.cs.columbia.edu/GenerAL/">Abhi Gupta<span class="glyphicon glyphicon-new-window" aria-hidden="true"></span></a>, 
    <a target="_blank" href="http://crlab.cs.columbia.edu/GenerAL/">Feng Xu<span class="glyphicon glyphicon-new-window" aria-hidden="true"></span></a>, 
    <a target="_blank" href="http://crlab.cs.columbia.edu/GenerAL/">Jacob Varley<span class="glyphicon glyphicon-new-window" aria-hidden="true"></span></a>, 
    <a target="_blank" href="http://crlab.cs.columbia.edu/GenerAL/">David Watkins-Valls<span class="glyphicon glyphicon-new-window" aria-hidden="true"></span></a>, and 
    <a target="_blank" href="http://www.cs.columbia.edu/~allen/">Peter Allen<span class="glyphicon glyphicon-new-window" aria-hidden="true"></span></a><br>
  </p><p>
  </p><p style="color:#aaa; margin-bottom: 20px">
  <a target="_blank" href="http://www.cs.columbia.edu/robotics/">Columbia Robotics Lab<span class="glyphicon glyphicon-new-window" aria-hidden="true"></span></a> and 
  <a target="_blank" href="https://ai.google/research/teams/brain/robotics/">Robotics at Google<span class="glyphicon glyphicon-new-window" aria-hidden="true"></span></a></p>
</center>

## Abstract
Generative Attention Learning (GenerAL) is a framework for high-DOF multi-fingered grasping that is not only robust to dense clutter and novel objects, but also effective with a variety of different parallel-jaw and multi-fingered robot hands. This framework introduces a novel attention mechanism that substantially improves grasp success rate in clutter. Its generative nature allows the learning of full-DOF grasps with flexible end-effector positions and orientations, as well as all finger joint angles of the hand. Trained purely in simulation, this framework closes the  visual sim-to-real gap by using a single depth image as input and closes the dynamics sim-to-real gap by circumventing continuous motor control with a direct mapping from pixel to Cartesian space inferred from the same depth image. Finally, this framework demonstrates inter-robot generality by achieving over 90% grasp success rates in cluttered scenes with novel objects using two multi-fingered robotic hand/arm systems with different degrees of freedom.

<hr />

## Full Video (4-Minutes)

<iframe width="854" height="480" src="https://www.youtube.com/embed/GROLYFve9Cc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## Acknowledgement

This work was supported in part by a Google Research grant and National Science Foundation grants CMMI-1734557 and IIS-1527747.

<hr />
